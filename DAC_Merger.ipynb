{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.17.55.5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>CMS-Refactor</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd71c2a7260>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CMS-Refactor\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "df4c7f82-fc4d-40ef-abe2-85b5dc796087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "phase_1_path = \"outputs/final_out.csv\"\n",
    "dac_path = \"../CMS-Classic/Data/Public/DAC_NationalDownloadableFile.csv\"\n",
    "\n",
    "phase_1 = spark.read.csv(\n",
    "    phase_1_path,\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "dac = spark.read.csv(\n",
    "    dac_path,\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "dac = dac.select([\n",
    "    \"NPI\",\n",
    "    \"Facility Name\",\n",
    "    \"org_pac_id\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "601d0f41-db04-4055-a233-0db11aeeff5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "dac_dropped = dac.dropna()\n",
    "dac_dropped = dac.dropDuplicates()\n",
    "\n",
    "# Convert org_pac_id to 'int'\n",
    "dac_dropped = dac_dropped.withColumn(\n",
    "    \"org_pac_id\",\n",
    "    floor(dac_dropped.org_pac_id)\n",
    ")\n",
    "\n",
    "grouped_df = dac_dropped.groupBy('NPI').agg(\n",
    "    collect_list(\"Facility Name\").alias(\"Facility_Names\"),\n",
    "    collect_list(\"org_pac_id\").alias(\"org_pac_ids\")\n",
    ")\n",
    "\n",
    "max_length = grouped_df.selectExpr(\"size(Facility_Names) as size\").rdd.max()[0]\n",
    "\n",
    "for i in range(max_length):\n",
    "    grouped_df = grouped_df.withColumn(f\"Facility_Name_{i+1}\", expr(f\"Facility_Names[{i}]\")).withColumn(f\"Org_pac_id_{i+1}\", expr(f\"org_pac_ids[{i}]\"))\n",
    "\n",
    "grouped_df = grouped_df.drop(\"Facility_Names\", \"org_pac_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "eedb3133-251b-4054-bb8e-cc40439753ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9439"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_inner = phase_1.join(\n",
    "    grouped_df,\n",
    "    \"NPI\",\n",
    "    \"inner\"\n",
    ")\n",
    "merged_inner.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8d76322a-a2b4-46d0-9f75-1279309c9499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10196"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_left = phase_1.join(\n",
    "    grouped_df,\n",
    "    \"NPI\",\n",
    "    \"left\"\n",
    ")\n",
    "merged_left.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "503b5ee0-3ec1-47a5-8588-3b0fe180ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange the columns\n",
    "columns = phase_1.columns[:27]\n",
    "columns.remove('_c0')\n",
    "g_cols = grouped_df.columns\n",
    "g_cols.remove('NPI')\n",
    "columns.extend(g_cols)\n",
    "columns.extend(phase_1.columns[27:])\n",
    "\n",
    "merged_inner = merged_inner.select(columns)\n",
    "merged_left = merged_left.select(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "75716a89-80e0-40ac-a6b7-48410463b056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "merged_inner.toPandas().to_csv(\"outputs/cms_dac_inner.csv\")\n",
    "merged_left.toPandas().to_csv(\"outputs/cms_dac_left.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
